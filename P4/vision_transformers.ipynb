{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# ROB 498-004/599-004 Assignment 4-2: Vision Transformers\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Anthony OPIPARI, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "tt7vq1h6mRto",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Answer:**   \n",
    "Your NAME, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "FrfeHl_-m4V-",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1739733339479,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "VyQblYp0nEZq",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "Q7ymI0aZ2W1b",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3519,
     "status": "ok",
     "timestamp": 1739733388080,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "c_LLpLyC2eau",
    "new_sheet": false,
    "outputId": "0df65b61-e568-404c-9bbe-114f217068ae",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "running_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if running_colab:\n",
    "    from google.colab import drive\n",
    "    print('Running on Colab')\n",
    "    drive.mount('/content/drive/', force_remount=True)\n",
    "else:\n",
    "    print('Running locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "mbq-UT8J2mnv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"pose_estimation.ipynb\", \"vision_transformers.ipynb\", \"rob599\", \"pose_estimation.py\", \"vision_transformers.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1739733402385,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "WcrhTOZW243H",
    "new_sheet": false,
    "outputId": "d5695c8b-d5eb-454d-b238-ee8139f5bae9",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2025WN folder and put all the files under P4 folder, then '2025WN/P4'\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2025WN/P4'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "xegb0uDA232J",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from convolutional_networks.py!\n",
    "Hello from p4_helper.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `convolutional_networks.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12608,
     "status": "ok",
     "timestamp": 1739733420335,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "AhGQF5sw3Fas",
    "new_sheet": false,
    "outputId": "20845421-ba4c-42ea-f995-5e0cc0a49090",
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "import time, os\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from vision_transformers import hello_vision_transformers\n",
    "hello_vision_transformers()\n",
    "\n",
    "from rob599.p4_helpers import hello_helper\n",
    "hello_helper()\n",
    "\n",
    "vision_transformers_path = os.path.join(GOOGLE_DRIVE_PATH, 'vision_transformers.py')\n",
    "vision_transformers_edit_time = time.ctime(os.path.getmtime(vision_transformers_path))\n",
    "print('vision_transformers.py last edited on %s' % vision_transformers_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ynKS05gJ4iBo",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "fN1SShPR4lJV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup code\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "VUCKw4Tl1ddj",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import rob599\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "from rob599 import reset_seed\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "lhqpd2IN2O-K",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1739650685888,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "SGDxdBIMRX6b",
    "new_sheet": false,
    "outputId": "98621c2f-f5e0-4afe-bf82-2d9aafc826ba",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "-Yv3zQYw5B3s",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load the Progress Objects dataset\n",
    "We will first load the Progress Objects dataset, just as we did in previous projects. The utility function `eecs598.data.preprocess_progress_objects()` returns the entire Progress Objects dataset as a set of six **Torch tensors** while also preprocessing the RGB images:\n",
    "\n",
    "- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n",
    "- `y_train` contains all training labels (integers in the range $[0, 9]$)\n",
    "- `X_val` contains all validation images\n",
    "- `y_val` contains all validation labels\n",
    "- `X_test` contains all test images\n",
    "- `y_test` contains all test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "executionInfo": {
     "elapsed": 11991,
     "status": "ok",
     "timestamp": 1739650699852,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "V2mFlFmQ1ddm",
    "new_sheet": false,
    "outputId": "5793cd4d-768f-439c-878c-dd07d4ebad60",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Invoke the above function to get our data.\n",
    "import rob599\n",
    "\n",
    "rob599.reset_seed(0)\n",
    "data_dict = rob599.data.preprocess_progress_objects(cuda=True, dtype=torch.float64, flatten=False)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "aQW_w1Wzw72f",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Transformer networks\n",
    "In the last project, we focused on convolutional networks which provided a major improvement in performance relative to compute when compared to fully-connected networks. More recently, vision transformers have proved to be useful across a range of perception tasks especially when pre-trained on large-scale datasets.\n",
    "\n",
    "First you will implement several layer types that are used in transformer networks. You will then use these layers to train a vision transformer network on the Progress Objects dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patchify Input Images\n",
    "\n",
    "Unlike convolutional networks, which process input images that are shaped as 3D tensors (N,C,H,W), transformer networks expect input in the form of image patches. First, we will implement the `patchify` function in the `vision_transformers.py` file to help with image preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import patchify\n",
    "\n",
    "x_shape = torch.tensor((2, 3, 4, 4))\n",
    "x = torch.linspace(-0.1, 0.5, steps=torch.prod(x_shape), dtype=torch.float64, device='cuda').reshape(*x_shape)\n",
    "patch_size = 2\n",
    "\n",
    "out = patchify(x, patch_size)\n",
    "correct_out = torch.tensor([[[-0.10000000,  0.00105263,  0.10210526, -0.09368421,  0.00736842,\n",
    "                               0.10842105, -0.07473684,  0.02631579,  0.12736842, -0.06842105,\n",
    "                               0.03263158,  0.13368421],\n",
    "                             [-0.08736842,  0.01368421,  0.11473684, -0.08105263,  0.02000000,\n",
    "                               0.12105263, -0.06210526,  0.03894737,  0.14000000, -0.05578947,\n",
    "                               0.04526316,  0.14631579],\n",
    "                             [-0.04947368,  0.05157895,  0.15263158, -0.04315789,  0.05789474,\n",
    "                               0.15894737, -0.02421053,  0.07684211,  0.17789474, -0.01789474,\n",
    "                               0.08315789,  0.18421053],\n",
    "                             [-0.03684211,  0.06421053,  0.16526316, -0.03052632,  0.07052632,\n",
    "                               0.17157895, -0.01157895,  0.08947368,  0.19052632, -0.00526316,\n",
    "                               0.09578947,  0.19684211]],\n",
    "                    \n",
    "                            [[ 0.20315789,  0.30421053,  0.40526316,  0.20947368,  0.31052632,\n",
    "                               0.41157895,  0.22842105,  0.32947368,  0.43052632,  0.23473684,\n",
    "                               0.33578947,  0.43684211],\n",
    "                             [ 0.21578947,  0.31684211,  0.41789474,  0.22210526,  0.32315789,\n",
    "                               0.42421053,  0.24105263,  0.34210526,  0.44315789,  0.24736842,\n",
    "                               0.34842105,  0.44947368],\n",
    "                             [ 0.25368421,  0.35473684,  0.45578947,  0.26000000,  0.36105263,\n",
    "                               0.46210526,  0.27894737,  0.38000000,  0.48105263,  0.28526316,\n",
    "                               0.38631579,  0.48736842],\n",
    "                             [ 0.26631579,  0.36736842,  0.46842105,  0.27263158,  0.37368421,\n",
    "                               0.47473684,  0.29157895,  0.39263158,  0.49368421,  0.29789474,\n",
    "                               0.39894737,  0.50000000]]],\n",
    "                          dtype=torch.float64, device='cuda',\n",
    "            )\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing patchify')\n",
    "print('difference: ', rob599.grad.rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Image Patches\n",
    "\n",
    "Once we have implemented our helper function, we can examine the image patch structure that will be given to our transformers using the following cell. You should see your patchified images 'align' with the input image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import patchify\n",
    "from rob599.utils import visualize_patches\n",
    "\n",
    "patch_size = 4\n",
    "x = data_dict['X_train'][:4]\n",
    "\n",
    "N, C, H, W = x.shape\n",
    "assert (N, C, H, W)==(4,3,32,32)\n",
    "\n",
    "\n",
    "out = patchify(x, patch_size)\n",
    "out = out.view(N, H//patch_size*W//patch_size, patch_size, patch_size, C)\n",
    "out = out.permute(0,1,4,2,3)\n",
    "\n",
    "fig = visualize_patches(x, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layers\n",
    "The attention layer was introduced by Vaswani et al. in their paper, \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" primarily as a mechanism to allow models to learn relationships across long temporal sequences in NLP. The resulting transformer architectures have drastically changed NLP and robot vision by enabling models including GPT and ViT. In the following sections, we will re-implement a [ViT model (Dosovitskiy et al.)](https://arxiv.org/abs/2010.11929) for use in object classification and self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by implementing a generic attention layer in `vision_transformers.py` followed by a multi-head self-attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention Layer\n",
    "\n",
    "Following the definition given in Eqn. (1) from [Vaswani et al.](https://arxiv.org/abs/1706.03762), scaled dot-product attention is defined over an input query $Q\\in\\mathbb{R}^{P\\times d_k}$, value $v\\in\\mathbb{R}^{n \\times d_k}$, and key $k \\in \\mathbb{R}^{n\\times d_v}$, given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Attention(Q,K,V)} = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_k}})V \\in \\mathbb{R}^{P\\times d_v} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For the purpose of vision transformers, $P$ represnents the number of patches given as input to the attention layer, $d_k$ represents the input dimension of our patches as queries, and $d_v$ represents the output dimension of the layer.\n",
    "\n",
    "To improve the performance and reduce overfitting, it is common to include a dropout layer after the attention mechanism:\n",
    "\\begin{align}\n",
    "\\text{Attention(Q,K,V)} = \\text{dropout}( \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_k}})V ) \\\\\n",
    "\\end{align}\n",
    "\n",
    "This layer is illustrated in Figure 2 of the paper, \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" by Vaswani et al. and is included below for your reference:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://deeprob.org/w25/assets/images/scaled_dot_product_attention.webp\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this context, implement the `Attention` layer class as defined in `vision_transformers.py`. Note that in this implementation we will assume that `embed_dim`$=d_k=d_v$. In addtion, note that we will use a linear projection layer on the input and output in order to have greater control on the total number of learnable parameters used by our layer.\n",
    "\n",
    "After implementing the attention layer, run the following cell to test your implementation. You should get errors less than `1e-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import Attention\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "q = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "k = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "v = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "\n",
    "attn = Attention(embed_dim=4, hidden_dim=4)\n",
    "attn = attn.cuda()\n",
    "\n",
    "self_attn_out, self_attn_prob_out = attn(q, q, q)\n",
    "attn_out, attn_prob_out = attn(q, k, v)\n",
    "\n",
    "self_attn_correct_out = torch.tensor([[[-0.10627611, -0.62611639,  0.13222384, -1.09313488],\n",
    "                                         [-0.10645102, -0.61488867,  0.10189836, -0.94813520],\n",
    "                                         [ 0.07612614, -0.54552132,  0.31153607, -0.82238555],\n",
    "                                         [ 0.06230495, -0.46409360, -0.03301117, -0.42071623]],\n",
    "                                \n",
    "                                        [[ 0.11057287, -0.39444178,  0.06920159, -0.08880806],\n",
    "                                         [ 0.19311240, -0.35335156,  0.18412718, -0.29377916],\n",
    "                                         [ 0.14516702, -0.35476482,  0.08905903, -0.20466976],\n",
    "                                         [ 0.11944707, -0.43682113,  0.07853142, -0.28273490]]],\n",
    "                               dtype=torch.float32, device='cuda',\n",
    "                )\n",
    "self_attn_prob_correct_out = torch.tensor([[[0.32628635, 0.25703996, 0.23558623, 0.18108749],\n",
    "                                             [0.24741820, 0.26384076, 0.22483547, 0.26390556],\n",
    "                                             [0.24048136, 0.42926881, 0.08706803, 0.24318181],\n",
    "                                             [0.13283394, 0.34724340, 0.09205462, 0.42786804]],\n",
    "                                    \n",
    "                                            [[0.25455216, 0.13611786, 0.31953868, 0.28979129],\n",
    "                                             [0.24402443, 0.32087967, 0.21028540, 0.22481048],\n",
    "                                             [0.20070031, 0.20209791, 0.28204846, 0.31515339],\n",
    "                                             [0.20488618, 0.26016372, 0.24924056, 0.28570953]]],\n",
    "                                dtype=torch.float32, device='cuda',\n",
    "                )\n",
    "         \n",
    "attn_correct_out = torch.tensor([[[ 0.05912217, -0.42211652, -0.05042682, -0.39600593],\n",
    "                                 [ 0.20306754, -0.32697055,  0.13665706, -0.43784353],\n",
    "                                 [ 0.20316094, -0.32098979,  0.05530454, -0.55322063],\n",
    "                                 [ 0.19546732, -0.32679880,  0.05251367, -0.54702628]],\n",
    "                        \n",
    "                                [[-0.02486524, -0.48058981,  0.01680537, -0.85303235],\n",
    "                                 [ 0.02255404, -0.59745276,  0.20221131, -0.80400515],\n",
    "                                 [ 0.03725577, -0.45910370, -0.14297450, -0.65868229],\n",
    "                                 [-0.04865351, -0.52705979, -0.04593496, -0.94209051]]],\n",
    "                          dtype=torch.float32, device='cuda',\n",
    "            )\n",
    "attn_prob_correct_out = torch.tensor([[[0.20586143, 0.25646833, 0.28275558, 0.25491467],\n",
    "                                         [0.22584742, 0.27487761, 0.25794467, 0.24133025],\n",
    "                                         [0.37149656, 0.28244114, 0.17336245, 0.17269991],\n",
    "                                         [0.36424279, 0.30884111, 0.16077881, 0.16613729]],\n",
    "                                \n",
    "                                        [[0.09529946, 0.20541951, 0.28147435, 0.41780668],\n",
    "                                         [0.33548340, 0.25171298, 0.21469083, 0.19811280],\n",
    "                                         [0.08185083, 0.23376857, 0.32362539, 0.36075523],\n",
    "                                         [0.13896324, 0.26203558, 0.30177021, 0.29723096]]],\n",
    "                                     dtype=torch.float32, device='cuda',\n",
    "            )\n",
    "                                     \n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing Attention')\n",
    "print('self-attention output difference: ', rob599.grad.rel_error(self_attn_out, self_attn_correct_out))\n",
    "print('self-attention probability map difference: ', rob599.grad.rel_error(self_attn_prob_out, self_attn_prob_correct_out))\n",
    "print()\n",
    "print('attention output difference: ', rob599.grad.rel_error(attn_out, attn_correct_out))\n",
    "print('attention probability map difference: ', rob599.grad.rel_error(attn_prob_out, attn_prob_correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Layer\n",
    "Next, we'll implement a multi-head attention layer in an effort to achieve more performant transformers. For multi-head attention we will split the embedding dimension, $d_{\\text{model}}$, into $h$ independent sub-spaces and perform an attention layer on each of the $h$ sub-spaces in parallel before recombining the final output. This process is defined according to the following rule:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{MultiHeadAttention(Q,K,V)} &= \\text{concat} (\\text{head}_1,\\ldots,\\text{head}_h) W^{O}\\\\\n",
    "&\\text{where} \\;\\; \\text{head}_i = \\text{dropout}( \\text{softmax}(\\frac{QW_{i}^{Q}{(KW_{i}^{K})}^{T}}{\\sqrt{\\frac{d_\\text{model}}{h}}})VW_{i}^{V} )\n",
    "\\end{align}\n",
    "\n",
    "Where the parameters $W_{i}^{Q}\\in\\mathbb{R}^{d_\\text{model}\\times d_k}$, $W_{i}^{K}\\in\\mathbb{R}^{d_\\text{model}\\times d_k}$, and $W_{i}^{V}\\in\\mathbb{R}^{d_\\text{model}\\times d_v}$ are learned parameter matrices that project the original input query, keys and values to the respective sub-spaces and $W^{O}\\in\\mathbb{R}^{h*d_{v}\\times d_\\text{model}}$ is a learned projection to the output space. \n",
    "\n",
    "This layer is illustrated in Figure 2 of the paper, \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" by Vaswani et al. and is included below for your reference:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://deeprob.org/w25/assets/images/multi_head_attention.webp\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the `MultiHeadAttention` layer class as defined in `vision_transformers.py`. After implementing the attention layer, run the following cell to test your implementation. You should get errors less than `1e-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import MultiHeadAttention\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "q = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "k = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "v = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "\n",
    "\n",
    "attn = MultiHeadAttention(embed_dim=4, hidden_dim=4, num_heads=2).to('cuda')\n",
    "\n",
    "\n",
    "self_attn_out, self_attn_prob_out = attn(query=q, key=q, value=q)\n",
    "attn_out, attn_prob_out = attn(query=q, key=k, value=v)\n",
    "\n",
    "self_attn_correct_out = torch.tensor([[[ 0.14430067, -0.53784740,  0.08220236, -0.62189162],\n",
    "                                         [-0.08875817, -0.63622594,  0.13209616, -0.96715879],\n",
    "                                         [ 0.01958477, -0.58957678,  0.32356644, -1.08062053],\n",
    "                                         [ 0.01057089, -0.44386402, -0.12629637, -0.41127431]],\n",
    "                                \n",
    "                                        [[ 0.08677953, -0.32324642,  0.04659149, -0.05140835],\n",
    "                                         [ 0.22325052, -0.44497642,  0.25980285, -0.43782300],\n",
    "                                         [ 0.22102278, -0.28993517,  0.18363853, -0.18710478],\n",
    "                                         [ 0.16332752, -0.32018024,  0.12435215, -0.15900445]]],\n",
    "                               dtype=torch.float32, device='cuda',\n",
    "                )\n",
    "self_attn_prob_correct_out = torch.tensor([[[[0.18017843, 0.20693181, 0.25834346, 0.35454631],\n",
    "                                          [0.22774993, 0.22802918, 0.26726842, 0.27695245],\n",
    "                                          [0.13108703, 0.37074625, 0.07695191, 0.42121479],\n",
    "                                          [0.20075037, 0.39590091, 0.10477507, 0.29857367]],\n",
    "                                \n",
    "                                         [[0.43813339, 0.27225250, 0.19278474, 0.09682937],\n",
    "                                          [0.26745394, 0.29254177, 0.19905598, 0.24094830],\n",
    "                                          [0.39628604, 0.31796229, 0.16045970, 0.12529200],\n",
    "                                          [0.13106003, 0.25865844, 0.14949824, 0.46078330]]],\n",
    "                                \n",
    "                                \n",
    "                                        [[[0.15665036, 0.28312963, 0.23444588, 0.32577407],\n",
    "                                          [0.37158445, 0.17279993, 0.26420507, 0.19141062],\n",
    "                                          [0.12101869, 0.35739523, 0.20069814, 0.32088789],\n",
    "                                          [0.17456272, 0.32275257, 0.22152309, 0.28116164]],\n",
    "                                \n",
    "                                         [[0.36663607, 0.08369717, 0.33788803, 0.21177872],\n",
    "                                          [0.14213750, 0.45017898, 0.16196926, 0.24571429],\n",
    "                                          [0.32986715, 0.11279890, 0.32183596, 0.23549798],\n",
    "                                          [0.26374865, 0.19997428, 0.27420843, 0.26206869]]]],\n",
    "                               dtype=torch.float32, device='cuda',\n",
    "                )\n",
    "\n",
    "attn_correct_out = torch.tensor([[[ 0.19000325, -0.40944189,  0.05954247, -0.42533216],\n",
    "                                 [ 0.20030579, -0.42216203,  0.11929678, -0.35989812],\n",
    "                                 [ 0.17565413, -0.31465685, -0.01690275, -0.51006687],\n",
    "                                 [ 0.24779554, -0.33915606,  0.20358221, -0.29212722]],\n",
    "                        \n",
    "                                [[-0.07406262, -0.49398190, -0.00068270, -0.92019606],\n",
    "                                 [ 0.01650470, -0.51480108, -0.18196332, -0.76464653],\n",
    "                                 [ 0.03981098, -0.52065486,  0.24446507, -0.70421875],\n",
    "                                 [-0.06706452, -0.51757574, -0.04265606, -0.95538819]]],\n",
    "                          dtype=torch.float32, device='cuda',\n",
    "            )\n",
    "attn_prob_correct_out = torch.tensor([[[[0.17551190, 0.30181104, 0.27964675, 0.24303028],\n",
    "                                  [0.21388243, 0.26558176, 0.26735255, 0.25318328],\n",
    "                                  [0.40423116, 0.29908296, 0.14499946, 0.15168637],\n",
    "                                  [0.44156876, 0.24115716, 0.14817984, 0.16909422]],\n",
    "                        \n",
    "                                 [[0.26640022, 0.21140006, 0.26191550, 0.26028419],\n",
    "                                  [0.25274917, 0.26874062, 0.24400392, 0.23450626],\n",
    "                                  [0.26538554, 0.24343665, 0.25178874, 0.23938912],\n",
    "                                  [0.23541282, 0.34134340, 0.22068055, 0.20256321]]],\n",
    "                        \n",
    "                        \n",
    "                                [[[0.06956770, 0.23741508, 0.40017304, 0.29284424],\n",
    "                                  [0.47028124, 0.18850142, 0.13698901, 0.20422834],\n",
    "                                  [0.06562116, 0.26101801, 0.42940381, 0.24395697],\n",
    "                                  [0.15108673, 0.27565616, 0.33205390, 0.24120323]],\n",
    "                        \n",
    "                                 [[0.21770586, 0.18900888, 0.17506647, 0.41821879],\n",
    "                                  [0.17917261, 0.29776230, 0.32718062, 0.19588445],\n",
    "                                  [0.18627781, 0.20657638, 0.19890791, 0.40823790],\n",
    "                                  [0.18043548, 0.24251601, 0.24581796, 0.33123049]]]],\n",
    "                          dtype=torch.float32, device='cuda',\n",
    "            )\n",
    "\n",
    "print('Testing Multi-Head Attention')\n",
    "print('self-attention output difference: ', rob599.grad.rel_error(self_attn_out, self_attn_correct_out))\n",
    "print('self-attention probability map difference: ', rob599.grad.rel_error(self_attn_prob_out, self_attn_prob_correct_out))\n",
    "print()\n",
    "print('attention output difference: ', rob599.grad.rel_error(attn_out, attn_correct_out))\n",
    "print('attention probability map difference: ', rob599.grad.rel_error(attn_prob_out, attn_prob_correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm\n",
    "Unlike convolutional neural networks, which typically use BatchNormalization, Vision Transformers commonly rely on LayerNormalization. We'll now implement the forward and backward pass of layer normalization following its definition in the paper by Ba et al., \"[Layer Normalization](https://arxiv.org/abs/1607.06450)\". Please work on your implementation in the provided `LayerNorm_fn` stencil within `vision_transformers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "from vision_transformers import LayerNorm_fn\n",
    "\n",
    "def print_mean_std(x,dim=0):\n",
    "  means = ['%.3f' % xx for xx in x.mean(dim=dim).tolist()]\n",
    "  stds = ['%.3f' % xx for xx in x.std(dim=dim).tolist()]\n",
    "  print('  means: ', means)\n",
    "  print('  stds:  ', stds)\n",
    "  print()\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "reset_seed(0)\n",
    "N, D1, D2, D3 = 2, 50, 60, 3\n",
    "X = torch.randn(N, N, D1, dtype=torch.float64, device='cuda')\n",
    "W1 = torch.randn(D1, D2, dtype=torch.float64, device='cuda')\n",
    "W2 = torch.randn(D2, D3, dtype=torch.float64, device='cuda')\n",
    "a = X.matmul(W1).clamp(min=0.).matmul(W2)\n",
    "\n",
    "print('Before layer normalization:')\n",
    "print_mean_std(a.flatten(start_dim=0, end_dim=1),dim=1)\n",
    "\n",
    "# Run with gamma=1, beta=0. Means should be close to zero and stds close to one\n",
    "gamma = torch.ones(D3, dtype=torch.float64, device='cuda')\n",
    "beta = torch.zeros(D3, dtype=torch.float64, device='cuda')\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = LayerNorm_fn.forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm.flatten(start_dim=0, end_dim=1),dim=1)\n",
    "\n",
    "# Run again with nontrivial gamma and beta. Now means should be close to beta\n",
    "# and std should be close to gamma.\n",
    "gamma = torch.tensor([3.0, 3.0, 3.0], dtype=torch.float64, device='cuda')\n",
    "beta = torch.tensor([12.0, 12.0, 12.0], dtype=torch.float64, device='cuda')\n",
    "print('After batch normalization (gamma=', gamma.tolist(), ', beta=', beta.tolist(), ')')\n",
    "a_norm, _ = LayerNorm_fn.forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm.flatten(start_dim=0, end_dim=1),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import LayerNorm_fn\n",
    "\n",
    "# Gradient check batchnorm backward pass\n",
    "reset_seed(0)\n",
    "N, D = 4, 5\n",
    "x = 5 * torch.randn(N, N, D, dtype=torch.float64, device='cuda') + 12\n",
    "gamma = torch.randn(D, dtype=torch.float64, device='cuda')\n",
    "beta = torch.randn(D, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(N, N, D, dtype=torch.float64, device='cuda')\n",
    "\n",
    "ln_param = {'mode': 'train'}\n",
    "fx = lambda x: LayerNorm_fn.forward(x, gamma, beta, ln_param)[0]\n",
    "fg = lambda a: LayerNorm_fn.forward(x, a, beta, ln_param)[0]\n",
    "fb = lambda b: LayerNorm_fn.forward(x, gamma, b, ln_param)[0]\n",
    "\n",
    "dx_num = rob599.grad.compute_numeric_gradient(fx, x, dout)\n",
    "da_num = rob599.grad.compute_numeric_gradient(fg, gamma.clone(), dout)\n",
    "db_num = rob599.grad.compute_numeric_gradient(fb, beta.clone(), dout)\n",
    "\n",
    "_, cache = LayerNorm_fn.forward(x, gamma, beta, ln_param)\n",
    "dx, dgamma, dbeta = LayerNorm_fn.backward(dout, cache)\n",
    "# You should expect to see relative errors between 1e-12 and 1e-9\n",
    "print('dx error: ', rob599.grad.rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rob599.grad.rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rob599.grad.rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder Layer\n",
    "We now have implemented all the pieces we need for a Transformer Encoder layer. For your implementation, please refer to the stencil provided in the `TransformerEncoder` class within the `vision_transformers.py` file.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://deeprob.org/w25/assets/images/vit_encoder.webp\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "After initializing the model's parameters and implementing its forward pass, run the cell below to verify your implementation. The following cell should result in errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import TransformerEncoder\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "x = torch.randn(2, 4, 4, dtype=torch.float32, device='cuda')\n",
    "\n",
    "\n",
    "tform = TransformerEncoder(embed_dim=4, hidden_dim=4, use_mhsa=True, num_heads=2, dropout=0.2).to('cuda')\n",
    "\n",
    "\n",
    "tform_out, tform_attn_prob_out = tform(x)\n",
    "\n",
    "tform_correct_out = torch.tensor([[[ 0.22837791,  0.48520821, -1.48722327,  0.77875745],\n",
    "                                  [ 1.36270440, -0.87343740,  0.61074507, -0.73775852],\n",
    "                                  [-0.63110828, -0.15224275, -0.88840133,  1.86860383],\n",
    "                                  [-0.22899762, -1.44999933,  1.40524316,  0.35814071]],\n",
    "                         \n",
    "                                 [[-0.91953140, -0.57453340,  1.45202363, -0.22946705],\n",
    "                                  [ 1.14850068, -1.10756433,  0.80179948, -0.46173581],\n",
    "                                  [-0.27686939, -1.28413272,  1.62888932, -0.05994554],\n",
    "                                  [ 0.19288762, -1.30245209,  1.53798258, -0.29093570]]],\n",
    "                               dtype=torch.float32, device='cuda',\n",
    "                )\n",
    "tform_attn_prob_correct_out = torch.tensor([[[[0.24556586, 0.23461305, 0.22854063, 0.29128045],\n",
    "                                               [0.26582325, 0.24528989, 0.25675115, 0.23213568],\n",
    "                                               [0.16706130, 0.22485907, 0.17249395, 0.43558577],\n",
    "                                               [0.13346814, 0.22852235, 0.15546216, 0.48254728]],\n",
    "                                     \n",
    "                                              [[0.24627049, 0.26261571, 0.23864307, 0.25247076],\n",
    "                                               [0.23378015, 0.28552386, 0.21900398, 0.26169202],\n",
    "                                               [0.24801090, 0.28813699, 0.21729313, 0.24655904],\n",
    "                                               [0.23613985, 0.33063841, 0.18558523, 0.24763654]]],\n",
    "                                     \n",
    "                                     \n",
    "                                             [[[0.21333496, 0.23287454, 0.26417810, 0.28961238],\n",
    "                                               [0.25751153, 0.26066428, 0.24502985, 0.23679435],\n",
    "                                               [0.18663345, 0.24879245, 0.26805818, 0.29651585],\n",
    "                                               [0.20335537, 0.25652543, 0.26213866, 0.27798051]],\n",
    "                                     \n",
    "                                              [[0.21793993, 0.29064220, 0.23361358, 0.25780430],\n",
    "                                               [0.19353986, 0.30574748, 0.23442140, 0.26629123],\n",
    "                                               [0.18919271, 0.32504857, 0.22164042, 0.26411831],\n",
    "                                               [0.18663532, 0.32387736, 0.22384013, 0.26564717]]]],\n",
    "                               dtype=torch.float32, device='cuda',\n",
    "                )\n",
    "\n",
    "\n",
    "print('Testing Transformer Encoder')\n",
    "print('transformer output difference: ', rob599.grad.rel_error(tform_out, tform_correct_out))\n",
    "print('transformer probability map difference: ', rob599.grad.rel_error(tform_attn_prob_out, tform_attn_prob_correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "Next, implement the `VisionTransformer` class in `vision_transformers.py`. Before working on this implementation, consider reading the original paper, \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" by Dosovitskiy et al. for useful details on the model's background and implementation.\n",
    "\n",
    "Figure 1 from the paper, \"[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\" by Dosovitskiy et al., may be a helpful reference and is included below:\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://deeprob.org/w25/assets/images/vit_arch.webp\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "After initializing the model's parameters and implementing its forward pass, run the cell below to verify your implementation. The following cell should result in errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import VisionTransformer\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "vit_model = VisionTransformer(\n",
    "        embed_dim = 8,\n",
    "        hidden_dim = 8,\n",
    "        num_layers = 1,\n",
    "        use_mhsa = True,\n",
    "        num_heads = 8,\n",
    "        input_dims = (3,8,8),\n",
    "        num_classes = 10,\n",
    "        patch_size = 4,\n",
    "        dropout = 0.1\n",
    "        )\n",
    "vit_model = vit_model.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randn(2,3,8,8).to('cuda')\n",
    "cls_tokens_out, patch_tokens_out, prob_map_out = vit_model(x)\n",
    "\n",
    "\n",
    "cls_tokens_correct = torch.tensor([[-0.08487247,  0.94621676,  0.21259612,  0.99615121, -0.66612917,\n",
    "                                     -0.09170625, -0.16521484,  0.41441488, -0.97881866, -0.79177248],\n",
    "                                    [-0.12858473,  0.95726472,  0.16973293,  0.95529091, -0.67910570,\n",
    "                                     -0.05758664, -0.11379831,  0.34818298, -1.04030991, -0.73268694]],\n",
    "                                   dtype=torch.float32, device='cuda')\n",
    "\n",
    "patch_tokens_correct = torch.tensor([[[ 0.26961550, -0.66625893, -0.41522637,  1.09102237, -0.17139906,\n",
    "                                          -1.59556139,  0.07700748,  1.47457910],\n",
    "                                         [ 1.40523458, -0.55241263,  0.78015375, -0.38925982,  0.67967224,\n",
    "                                           1.40331721, -0.44048807, -0.76978517],\n",
    "                                         [-1.71173632,  0.84137619,  0.13000342,  1.80485439, -0.11396644,\n",
    "                                          -1.45090199,  0.53379393,  0.11031771],\n",
    "                                         [ 1.51366949, -0.97072232, -0.78473318, -0.30233395,  0.91011310,\n",
    "                                           1.24063993,  0.74257815, -0.34607267],\n",
    "                                         [-1.00071716,  1.41462135,  0.41313174, -0.41611111, -1.26784325,\n",
    "                                          -0.03329726, -0.10818781,  2.14279652]],\n",
    "                                \n",
    "                                        [[ 0.31358403, -0.59571040, -0.44242427,  1.01546955, -0.12174353,\n",
    "                                          -1.58816493, -0.04374302,  1.51352358],\n",
    "                                         [ 1.24628484, -0.88689780, -0.99931216, -0.23807180,  1.54572940,\n",
    "                                           1.57659340, -0.45147681, -0.19232824],\n",
    "                                         [ 0.95628941,  0.24360117, -0.21780124,  1.95891714, -1.38544536,\n",
    "                                           0.89225626, -1.92898726,  0.06972896],\n",
    "                                         [-0.54251933, -0.49246719, -0.70089996, -0.03664243,  0.96426439,\n",
    "                                           2.31069112,  0.71305811, -0.53105533],\n",
    "                                         [-1.62545490,  0.21655037,  0.42078006, -0.07807606, -0.81975400,\n",
    "                                           0.64410102,  0.58329791,  2.14204049]]],\n",
    "                                     dtype=torch.float32, device='cuda')\n",
    "\n",
    "print('Testing Vision Transformer Model')\n",
    "print('cls tokens difference: ', rob599.grad.rel_error(cls_tokens_out, cls_tokens_correct))\n",
    "print('patch tokens difference: ', rob599.grad.rel_error(patch_tokens_out, patch_tokens_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "We're now ready to train our very own vision transformer. The following cells will setup the necessary datasets and dataloaders for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 16\n",
    "PATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data\n",
    "As with convolutional neural networks, a good starting point after implementing a transformer is to try training your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rob599 import Solver\n",
    "from vision_transformers import VisionTransformer, find_overfit_parameters\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data_dict['X_train'][:num_train],\n",
    "  'y_train': data_dict['y_train'][:num_train],\n",
    "  'X_val': data_dict['X_val'],\n",
    "  'y_val': data_dict['y_val'],\n",
    "}\n",
    "\n",
    "embed_dim, hidden_dim, num_layers, use_mhsa, num_heads, weight_decay, learning_rate = find_overfit_parameters()\n",
    "\n",
    "\n",
    "vit_model = VisionTransformer(\n",
    "    embed_dim = embed_dim,\n",
    "    hidden_dim = hidden_dim,\n",
    "    num_layers = num_layers,\n",
    "    use_mhsa = use_mhsa,\n",
    "    num_heads = num_heads,\n",
    "    input_dims = (3,32,32),\n",
    "    num_classes = NUM_CLASSES,\n",
    "    patch_size = PATCH_SIZE,\n",
    "    dropout = 0.1\n",
    "    ).to('cuda')\n",
    "\n",
    "solver = Solver(vit_model, small_data,\n",
    "                learning_rate=learning_rate,\n",
    "                weight_decay=weight_decay,\n",
    "                  num_epochs=50, batch_size=16,\n",
    "                  print_every=100, device='cuda')\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell, your model should show a converged training loss with high training accuracy (100%) with low validation accuracy (below 60%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training losses')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.gcf().set_size_inches(9, 4)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val accuracies')\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.gcf().set_size_inches(9, 4)\n",
    "plt.show()\n",
    "\n",
    "print('Final Training Accuracy:', solver.train_acc_history[-1])\n",
    "print('Final Validation Accuracy:', solver.val_acc_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Use of single- and multi-head Attention\n",
    "The following cell will evaluate your network on a subset of the PROPS dataset with a parameter sweep over your single and multi-head self attention layers. You should expect to see all the models achieving a >70% validation accuracy and a clear trend showing multi-head attention layers outperform the single attention layer and using more heads (up to 8) leads to higher validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rob599 import Solver\n",
    "from vision_transformers import VisionTransformer\n",
    "\n",
    "# Try training a ViT net with different attention methods\n",
    "num_train = 10000\n",
    "small_data = {\n",
    "  'X_train': data_dict['X_train'][:num_train],\n",
    "  'y_train': data_dict['y_train'][:num_train],\n",
    "  'X_val': data_dict['X_val'],\n",
    "  'y_val': data_dict['y_val'],\n",
    "}\n",
    "\n",
    "mhsa_params = [(False, 1), (True,2), (True,4), (True,8)]\n",
    "\n",
    "solvers = []\n",
    "for use_mhsa, num_heads in mhsa_params:\n",
    "    vit_model = VisionTransformer(\n",
    "        embed_dim = 16,\n",
    "        hidden_dim = 16,\n",
    "        num_layers = 1,\n",
    "        use_mhsa = use_mhsa,\n",
    "        num_heads = num_heads,\n",
    "        input_dims = (3,32,32),\n",
    "        num_classes = NUM_CLASSES,\n",
    "        patch_size = PATCH_SIZE,\n",
    "        dropout = 0.1\n",
    "        ).to('cuda')\n",
    "    \n",
    "    solver = Solver(vit_model, small_data,\n",
    "                  num_epochs=25, batch_size=128,\n",
    "                  print_every=100, device='cuda')\n",
    "    solver.train()\n",
    "    solvers.append(solver)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history_init(title, xlabel, solvers, labels, plot_fn, param_label, marker='-o'):\n",
    "  plt.title(title)\n",
    "  plt.xlabel(xlabel)\n",
    "  for solver, label in zip(solvers, labels):\n",
    "    data = plot_fn(solver)\n",
    "    label = param_label + '=' + str(label)\n",
    "    plt.plot(data, marker, label=label)\n",
    "  plt.legend(loc='lower center', ncol=len(solvers))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plot_training_history_init('Training loss','Iteration', solvers, mhsa_params,\n",
    "                            lambda x: x.loss_history, param_label='MHSA Head Params', marker='o')\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_training_history_init('Training accuracy','Epoch', solvers, mhsa_params,\n",
    "                           lambda x: x.train_acc_history, param_label='MHSA Head Params')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_training_history_init('Validation accuracy','Epoch', solvers, mhsa_params,\n",
    "                           lambda x: x.val_acc_history, param_label='MHSA Head Params')\n",
    "plt.gcf().set_size_inches(15, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Number of Layers\n",
    "The following cell will evaluate your network on a subset of the PROPS dataset with a parameter sweep over the number of MHSA layers. You should expect to see all the models achieving a >80% validation accuracy and a clear trend showing models with at least four layers outperforming those with fewer as measured by validation-set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rob599 import Solver\n",
    "from vision_transformers import VisionTransformer\n",
    "\n",
    "# Try training a ViT net with different attention methods\n",
    "num_train = 10000\n",
    "small_data = {\n",
    "  'X_train': data_dict['X_train'][:num_train],\n",
    "  'y_train': data_dict['y_train'][:num_train],\n",
    "  'X_val': data_dict['X_val'],\n",
    "  'y_val': data_dict['y_val'],\n",
    "}\n",
    "\n",
    "layer_params = [1, 2, 4, 8]\n",
    "\n",
    "solvers = []\n",
    "for num_layers in layer_params:\n",
    "    vit_model = VisionTransformer(\n",
    "        embed_dim = 16,\n",
    "        hidden_dim = 16,\n",
    "        num_layers = num_layers,\n",
    "        use_mhsa = True,\n",
    "        num_heads = 8,\n",
    "        input_dims = (3,32,32),\n",
    "        num_classes = NUM_CLASSES,\n",
    "        patch_size = PATCH_SIZE,\n",
    "        dropout = 0.1\n",
    "        ).to('cuda')\n",
    "    \n",
    "    solver = Solver(vit_model, small_data,\n",
    "                  num_epochs=25, batch_size=128,\n",
    "                  print_every=100, device='cuda')\n",
    "    solver.train()\n",
    "    solvers.append(solver)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plot_training_history_init('Training loss','Iteration', solvers, layer_params,\n",
    "                            lambda x: x.loss_history, param_label='Layers', marker='o')\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_training_history_init('Training accuracy','Epoch', solvers, layer_params,\n",
    "                           lambda x: x.train_acc_history, param_label='Layers')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_training_history_init('Validation accuracy','Epoch', solvers, layer_params,\n",
    "                           lambda x: x.val_acc_history, param_label='Layers')\n",
    "plt.gcf().set_size_inches(15, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best convolutional model that you can on PROPS, storing your best model in the `best_model` variable. We require you to get at least 71% accuracy on the validation set using a convolutional net, within 60 seconds of training.\n",
    "\n",
    "**Implement** `create_vit_solver_instance` while making sure to use the initialize your model with the input `dtype` and `device`, as well as initializing the solver on the input `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformers import VisionTransformer, create_vit_solver_instance\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "solver = create_vit_solver_instance(data_dict, torch.float32, \"cuda\")\n",
    "\n",
    "solver.train(time_limit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model!\n",
    "Run your best model on the validation and test sets. You should achieve above 71% accuracy on the validation set and 70% accuracy on the test set.\n",
    "\n",
    "(Our best model gets 99.0% validation accuracy and 70.3% test accuracy -- can you beat ours?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation set accuracy: ', solver.check_accuracy(data_dict['X_val'], data_dict['y_val']))\n",
    "print('Test set accuracy: ', solver.check_accuracy(data_dict['X_test'], data_dict['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(GOOGLE_DRIVE_PATH, 'one_minute_vit.pth')\n",
    "solver.model.save(path)\n",
    "\n",
    "# Create a new instance\n",
    "from vision_transformers import VisionTransformer, create_vit_solver_instance\n",
    "\n",
    "solver = create_vit_solver_instance(data_dict, torch.float32, \"cuda\")\n",
    "\n",
    "# Load model\n",
    "solver.model.load(path, dtype=torch.float32, device='cuda')\n",
    "\n",
    "# Evaluate on validation set\n",
    "print('Validation set accuracy: ', solver.check_accuracy(data_dict['X_val'], data_dict['y_val']))\n",
    "print('Test set accuracy: ', solver.check_accuracy(data_dict['X_test'], data_dict['y_test']))\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Attention Maps\n",
    "Before submitting our project, let's try to introspect our network to build an understanding of which parts of the image our vision transformer is 'attending' to when making its predictions. To do so, we will use the attnention maps returned by the network. Since each layer of our network uses self-attention, the output attention maps provide a measure of how the input feature tokens at each layer are being weighted to form the respective layers' output tokens. As a result, by taking the attention values that correspond to the output `cls_token`, we can visualize the weight given to each input tokens which provides some insight into which tokens contribute to the final prediction. In order to use this technique across multiple transformer layers, we use the Attention Rollout technique proposed by Abnar et al., which is beyond the scope of this project.\n",
    "\n",
    "To visualize the attention maps of your trained vision transformer, we have provided all the necessary visualization code for you in the helper file `rob599.utils.py`. If you are curious, feel free to look at the `attention_rollout` function or the paper by Abnar et al., \"[Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vision_transformers import patchify\n",
    "from rob599.utils import visualize_attention\n",
    "\n",
    "reset_seed(100)\n",
    "\n",
    "num_samples = 4\n",
    "samples = np.random.choice(data_dict['X_test'].shape[0], size=num_samples, replace=False)\n",
    "x = data_dict['X_test'][samples]\n",
    "y = data_dict['y_test'][samples]\n",
    "\n",
    "N, C, H, W = x.shape\n",
    "assert (N, C, H, W)==(4,3,32,32)\n",
    "\n",
    "solver.model.eval()\n",
    "out_cls, out_tokens, out_attention = solver.model(x.to(dtype=solver.model.dtype, device=solver.model.in_proj.weight.device))\n",
    "\n",
    "fig = visualize_attention(x, out_attention, y, out_cls.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parts of the object is your network paying attention to most? Feel free to change the random seed to try visualizing the attention maps for different input images. How do the attention maps appear when the object is fully visible? What about under partial or full occlusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6Ule1zePwmR"
   },
   "source": [
    "## Verifying notebook cells\n",
    "\n",
    "Before moving onto the next part of the project, we can verify that no\n",
    "unexpected cells have been added or removed by **saving this file** then using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2337,
     "status": "ok",
     "timestamp": 1739733422668,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "JlNtaHVrPwmS",
    "outputId": "69589a28-f3bf-4c18-c3bd-e0f068679143"
   },
   "outputs": [],
   "source": [
    "from rob599.p4_helpers import verify_notebook_cells\n",
    "\n",
    "notebook_path = os.path.join(GOOGLE_DRIVE_PATH, 'vision_transformers.ipynb')\n",
    "verify_notebook_cells(notebook_path, expected_count=62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Your Work\n",
    "After completing this notebook, run the following cell to create a `.zip` file for you to download and turn in. \n",
    "\n",
    "**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rob599.submit import make_p4_submission\n",
    "\n",
    "# TODO: Replace these with your actual uniquename and umid\n",
    "uniquename = None\n",
    "umid = None\n",
    "\n",
    "make_p4_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnRCQDTFWbwn"
   },
   "source": [
    "Now we can submit our project to the autograder!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "P4-torch",
   "language": "python",
   "name": "p4-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
